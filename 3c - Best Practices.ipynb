{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20de159e-3124-46c1-86c5-a0632addaf48",
   "metadata": {},
   "source": [
    "Array traps and optimisation\n",
    "============================\n",
    "\n",
    "First let's start 3 workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224eb7dd-149c-4452-8080-d66d89c7a2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "from dask.distributed import Client, progress\n",
    "client = Client(\n",
    "    processes=False,\n",
    "    n_workers=3,\n",
    "    threads_per_worker=1,\n",
    ")\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f00e9d5-a90b-423a-b523-0fc4c8d9aaf8",
   "metadata": {},
   "source": [
    "Chunk size\n",
    "----------\n",
    "\n",
    "- Too large chunks don't split work efficiently.\n",
    "- Too small and too much time is lost in communication and other overheads.\n",
    "- 100Mb~1Gb per chunks is usually good. Scheduling a single task takes around ~1ms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df923b25-c7f1-4b51-8f38-3495fbae401d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"1 chunk\")\n",
    "arr = da.random.random((6000, 6000), chunks=(6000, 6000))\n",
    "%time x = arr.sum().compute()\n",
    "print(\"3 chunks\")\n",
    "arr = da.random.random((6000, 6000), chunks=(2000, 6000))\n",
    "%time x = arr.sum().compute()\n",
    "print(\"4 chunks\")\n",
    "arr = da.random.random((6000, 6000), chunks=(3000, 3000))\n",
    "%time x = arr.sum().compute()\n",
    "print(\"36 chunks\")\n",
    "arr = da.random.random((6000, 6000), chunks=(1000, 1000))\n",
    "%time x = arr.sum().compute()\n",
    "print(\"400 chunks\")\n",
    "arr = da.random.random((6000, 6000), chunks=(300, 300))\n",
    "%time x = arr.sum().compute()\n",
    "print(\"auto\")\n",
    "arr = da.random.random((6000, 6000), chunks=\"auto\")\n",
    "%time x = arr.sum().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8982c902-0191-4823-a109-6b88e690b168",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249041aa-174f-4469-9cf0-6d09a61486c2",
   "metadata": {},
   "source": [
    "Operation order\n",
    "---------------\n",
    "\n",
    "Dask has a symbolic tree of operations, but little tools for optimization.  \n",
    "It does not reorder operations for faster computation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a19d02a-90db-4896-bb06-d1c15f7351cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = da.random.random((3000, 3000), chunks=(1000, 1000))\n",
    "B = da.random.random((3000, 3000), chunks=(1000, 1000))\n",
    "v = da.random.random((3000, 1), chunks=(1000, 1))\n",
    "MM1 = (A @ B) @ v\n",
    "MM2 = A @ (B @ v)\n",
    "%time MM1.compute()\n",
    "%time MM2.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11579bfd-b503-48f5-8bb7-e26e900f9ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# einsum can do operation in the right order, but the operation is not optimized\n",
    "einMM = da.einsum(\"ij,jk,kl->il\", A, B, v)\n",
    "%time einMM.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adac5b18-251a-4e49-a92a-935725c7d401",
   "metadata": {},
   "source": [
    "Chunks alignment\n",
    "----------------\n",
    "Operation between array of any chunks will work.  \n",
    "However, when chunks are not matching, dask must re-chunks the arrays before doing the operation.  \n",
    "This slows down the computation significantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa950d73-e212-47c1-8d02-dea4bfdad6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = da.random.random((3000, 3000), chunks=(750, 750))\n",
    "B = da.random.random((3000, 3000), chunks=(600, 600))\n",
    "\n",
    "print(\"Same chunks\")\n",
    "%time A + A\n",
    "%time B + B\n",
    "\n",
    "print(\"Mixed chunks\")\n",
    "%time A + B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d483e776-c5d9-4863-830e-f783481cede3",
   "metadata": {},
   "source": [
    "Usage with other packages\n",
    "-------------------------\n",
    "Function passed to `map_block` or `delayed` can be optimized with other tools!  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9becef-e104-44b9-a4ac-c03e01714954",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask import delayed\n",
    "import numba\n",
    "import numpy as np\n",
    "\n",
    "@numba.jit\n",
    "def convolve2d(a):\n",
    "    return a[2:, 1:-1] + a[:-2, 1:-1] + a[1:-1, 2:] + a[1:-1, :-2] - 4 * a[1:-1, 1:-1]\n",
    "\n",
    "f = delayed(convolve2d)(np.ones((10, 10)))\n",
    "f.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f50544-f2ba-4e64-8060-174b6465a751",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = da.from_array(np.arange(100).reshape((10, 10)), chunks=(10, 10))\n",
    "out = da.overlap.map_overlap(convolve2d, x, depth=(1, 1), boundary=(\"reflect\", \"reflect\"))\n",
    "out.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c211bd-5707-4846-94f4-52e90cce9649",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
